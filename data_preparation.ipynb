{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIZyh7RDpJC8"
      },
      "source": [
        "# Merge SportVU and PBP Data for LSTM Shot Prediction\n",
        "This notebook downloads SportVU data, merges it with NBA Play-by-Play (PBP) data, and prepares a dataset for LSTM shot prediction.\n",
        "\n",
        "## Steps:\n",
        "1. Install dependencies.\n",
        "2. Import libraries and define constants.\n",
        "3. Download the zipped SportVU data.\n",
        "4. Unzip the data.\n",
        "5. Define utility functions for processing.\n",
        "6. Merge and process the data.\n",
        "7. Run the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eFd7VU-vY-X",
        "outputId": "54c07f0e-1266-41bd-d984-4312593296e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py7zr\n",
            "  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting texttable (from py7zr)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting pycryptodomex>=3.16.0 (from py7zr)\n",
            "  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pyzstd>=0.15.9 (from py7zr)\n",
            "  Downloading pyzstd-0.16.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n",
            "  Downloading pyppmd-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n",
            "  Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting multivolumefile>=0.2.3 (from py7zr)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n",
            "  Downloading inflate64-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting brotli>=1.1.0 (from py7zr)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from py7zr) (5.9.5)\n",
            "Downloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inflate64-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyppmd-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzstd-0.16.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n",
            "Successfully installed brotli-1.1.0 inflate64-1.0.1 multivolumefile-0.2.3 py7zr-0.22.0 pybcj-1.0.6 pycryptodomex-3.22.0 pyppmd-1.1.1 pyzstd-0.16.2 texttable-1.7.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install Dependencies\n",
        "!pip install py7zr\n",
        "!apt-get install -y p7zip-full  # Install 7z command-line tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-_YdNuZZpJDD"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Import Libraries and Define Constants\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "import random\n",
        "import zipfile\n",
        "import os\n",
        "import subprocess  # Added for running 7z command\n",
        "import shutil      # Added for directory cleanup\n",
        "\n",
        "# API headers for NBA stats API\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "    \"Referer\": \"https://stats.nba.com/\",\n",
        "    \"Accept\": \"application/json\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "}\n",
        "\n",
        "# Configuration\n",
        "INPUT_DIR = \"/content/sportvu_data/\"\n",
        "INPUT_FILE_DIR = \"/content/sportvu_data/2016.NBA.Raw.SportVU.Game.Logs\"\n",
        "OUTPUT_DIR = \"/content/merged_data/\"\n",
        "TEMP_DIR = \"/content/temp_extract/\"  # Added for temporary extraction directory\n",
        "NUM_MATCHES = 200  # Number of matches to sample\n",
        "MAX_SHOTS = 7000  # Maximum number of shots to extract\n",
        "WINDOW = 1.5  # Time window in seconds (0.5s = ~13 frames at 25 Hz)\n",
        "# Note: For Task 3 (optimal trajectories), increase WINDOW to 2.0 seconds (~50 frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QwhNpc3pJDE",
        "outputId": "0f27b921-e962-4884-bbff-a11ac1b6eab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1HFm6KKRVD5SGZZ3YzlkdVBPnU0C2ogE5\n",
            "From (redirected): https://drive.google.com/uc?id=1HFm6KKRVD5SGZZ3YzlkdVBPnU0C2ogE5&confirm=t&uuid=8089e2f0-54ce-4501-b2f3-464d16f897be\n",
            "To: /content/sportvu_all.zip\n",
            " 15% 570M/3.79G [00:08<00:47, 67.3MB/s]"
          ]
        }
      ],
      "source": [
        "# Cell 3: Download Data\n",
        "!gdown --id 1HFm6KKRVD5SGZZ3YzlkdVBPnU0C2ogE5 --output /content/sportvu_all.zip\n",
        "print(\"Data downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2AbguCwpJDF"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Unzip Data\n",
        "# Create input directory\n",
        "os.makedirs(INPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(\"/content/sportvu_all.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(INPUT_DIR)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Data unzipped successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jHUtiTypJDF"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Define Utility Functions\n",
        "\n",
        "def extract_gameid(sportvu_data):\n",
        "    \"\"\"Extract gameid from SportVU JSON data.\"\"\"\n",
        "    try:\n",
        "        return sportvu_data.get(\"gameid\", None)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_clock(clock_str):\n",
        "    \"\"\"Convert PBP clock (e.g., PT12M00.00S) to seconds.\"\"\"\n",
        "    try:\n",
        "        if \"PT\" in clock_str:\n",
        "            minutes = int(clock_str[2:4])\n",
        "            seconds = float(clock_str[5:10])\n",
        "        else:\n",
        "            minutes, seconds = map(float, clock_str.split(\":\"))\n",
        "        return minutes * 60 + seconds\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"Utility functions defined successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG_sPrHCPZEM"
      },
      "outputs": [],
      "source": [
        "def extract_sequence(moments, target_gameclock, period, window=WINDOW, game_stats=None):\n",
        "    \"\"\"Extract a sequence of moments within a time window before the target gameclock, with a buffer.\"\"\"\n",
        "    if game_stats is None:\n",
        "        game_stats = {}\n",
        "\n",
        "    # Initialize counters if not present\n",
        "    game_stats[\"total_shots\"] = game_stats.get(\"total_shots\", 0) + 1\n",
        "\n",
        "    if not moments or not isinstance(moments, list):\n",
        "        game_stats[\"no_moments\"] = game_stats.get(\"no_moments\", 0) + 1\n",
        "        return None, None, game_stats\n",
        "\n",
        "    # Convert moments list to a DataFrame\n",
        "    moments_df = pd.DataFrame(moments, columns=[\"period\", \"timestamp\", \"gameclock\", \"shotclock\", \"unknown\", \"positions\"])\n",
        "    if moments_df.empty:\n",
        "        game_stats[\"empty_df\"] = game_stats.get(\"empty_df\", 0) + 1\n",
        "        return None, None, game_stats\n",
        "\n",
        "    # Store initial count of rows\n",
        "    initial_count = len(moments_df)\n",
        "    # Convert gameclock to numeric and drop NaNs\n",
        "    moments_df[\"gameclock\"] = pd.to_numeric(moments_df[\"gameclock\"], errors='coerce')\n",
        "    moments_df = moments_df.dropna(subset=[\"gameclock\"])\n",
        "\n",
        "    # Calculate and log dropped rows\n",
        "    dropped_count = initial_count - len(moments_df)\n",
        "    if dropped_count > 0:\n",
        "        game_stats[\"nan_gameclock\"] = game_stats.get(\"nan_gameclock\", 0) + 1\n",
        "        game_stats[\"nan_gameclock_rows\"] = game_stats.get(\"nan_gameclock_rows\", 0) + dropped_count\n",
        "        return None, None, game_stats\n",
        "\n",
        "    if moments_df.empty:\n",
        "        game_stats[\"all_nan_gameclock\"] = game_stats.get(\"all_nan_gameclock\", 0) + 1\n",
        "        return None, None, game_stats\n",
        "\n",
        "    # Use gameclock for the window (in seconds)\n",
        "    buffer = 0.08  # 0.08 seconds (2 frames at 25 Hz)\n",
        "    end_gameclock = target_gameclock - buffer  # Subtract buffer because gameclock decreases\n",
        "    start_gameclock = end_gameclock + window + buffer  # Add window and buffer to get the start\n",
        "\n",
        "    # Filter moments within the gameclock window\n",
        "    sequence_df = moments_df[\n",
        "        (moments_df[\"gameclock\"] <= start_gameclock) &\n",
        "        (moments_df[\"gameclock\"] >= end_gameclock)\n",
        "    ].sort_values(\"gameclock\", ascending=False)  # Sort descending so most recent (lowest gameclock) is last\n",
        "\n",
        "    if sequence_df.empty:\n",
        "        game_stats[\"no_moments_in_window\"] = game_stats.get(\"no_moments_in_window\", 0) + 1\n",
        "        return None, None, game_stats\n",
        "\n",
        "    orig_len_sequence = len(sequence_df)\n",
        "    # Save original number of frames\n",
        "    game_stats[\"orig_frame_counts\"] = game_stats.get(\"orig_frame_counts\", []) + [orig_len_sequence]\n",
        "\n",
        "    # Drop near-duplicate frames based on same gameclock (keep only first per gameclock tick)\n",
        "    sequence_df = sequence_df.drop_duplicates(subset=[\"gameclock\"])\n",
        "    dropped_seq = orig_len_sequence - len(sequence_df)\n",
        "    if dropped_seq > 0:\n",
        "        game_stats[\"duplicate_frames\"] = game_stats.get(\"duplicate_frames\", 0) + 1\n",
        "        game_stats[\"duplicate_frames_dropped\"] = game_stats.get(\"duplicate_frames_dropped\", 0) + dropped_seq\n",
        "        return None, None, game_stats\n",
        "\n",
        "    # Require at least 10 moments (out of expected frames based on window size)\n",
        "    expected_frames = int(window * 25) + 1  # e.g., 38 frames for 1.5 seconds at 25 fps\n",
        "    if len(sequence_df) < expected_frames:\n",
        "        game_stats[\"insufficient_length\"] = game_stats.get(\"insufficient_length\", 0) + 1\n",
        "        game_stats[\"insufficient_length_counts\"] = game_stats.get(\"insufficient_length_counts\", []) + [len(sequence_df)]\n",
        "        return None, None, game_stats\n",
        "\n",
        "    # Take the last expected_frames moments (if more than expected, trim to expected)\n",
        "    sequence_df = sequence_df.tail(expected_frames)\n",
        "    game_stats[\"used_shots\"] = game_stats.get(\"used_shots\", 0) + 1\n",
        "    game_stats[\"final_frame_counts\"] = game_stats.get(\"final_frame_counts\", []) + [len(sequence_df)]\n",
        "\n",
        "    ball_seq = sequence_df[\"positions\"].apply(lambda x: x[0][2:]).tolist()  # [x, y, z]\n",
        "    players_seq = sequence_df[\"positions\"].apply(lambda x: x[1:]).tolist()  # [[teamid, playerid, x, y, z], ...]\n",
        "    return ball_seq, players_seq, game_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IyQQ5c8zVet"
      },
      "outputs": [],
      "source": [
        "def load_pbp_shots(gameid):\n",
        "    \"\"\"Fetch PBP data for a given gameid using the playbyplayv3 API and extract shot events.\"\"\"\n",
        "    url = \"https://stats.nba.com/stats/playbyplayv3\"\n",
        "    params = {\n",
        "        \"GameID\": gameid,\n",
        "        \"StartPeriod\": 0,\n",
        "        \"EndPeriod\": 14\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        pbp_data = response.json()\n",
        "\n",
        "        # Check if the expected structure exists\n",
        "        if not pbp_data or \"game\" not in pbp_data or \"actions\" not in pbp_data[\"game\"]:\n",
        "            print(f\"No PBP data returned for gameid: {gameid}\")\n",
        "            return None\n",
        "\n",
        "        # Extract actions directly from pbp_data[\"game\"][\"actions\"]\n",
        "        actions = pbp_data[\"game\"][\"actions\"]\n",
        "        if not actions:\n",
        "            print(f\"No actions found in PBP data for gameid: {gameid}\")\n",
        "            return None\n",
        "\n",
        "        # Convert actions to DataFrame\n",
        "        df_pbp = pd.DataFrame(actions)\n",
        "\n",
        "        # Define required columns for processing\n",
        "        required_cols = [\"actionNumber\", \"period\", \"clock\", \"isFieldGoal\", \"shotResult\", \"personId\", \"shotDistance\"]\n",
        "        optional_cols = [\"xLegacy\", \"yLegacy\", \"shotValue\", \"teamId\"]  # Optional but useful columns\n",
        "        missing_required_cols = [col for col in required_cols if col not in df_pbp.columns]\n",
        "        if missing_required_cols:\n",
        "            print(f\"Missing required columns in PBP data for gameid {gameid}: {missing_required_cols}\")\n",
        "            return None\n",
        "\n",
        "        # Filter for shots (isFieldGoal = 1)\n",
        "        df_shots = df_pbp[df_pbp[\"isFieldGoal\"] == 1].copy()\n",
        "        if df_shots.empty:\n",
        "            print(f\"No shot events found in PBP data for gameid: {gameid}\")\n",
        "            return None\n",
        "\n",
        "        # Validate that only shot events are included\n",
        "        invalid_shots = df_shots[~df_shots[\"shotResult\"].isin([\"Made\", \"Missed\"])]\n",
        "        if not invalid_shots.empty:\n",
        "            print(f\"Warning: Invalid shot results in PBP data for gameid {gameid}: {invalid_shots['shotResult'].unique()}\")\n",
        "\n",
        "        # Map columns to expected format\n",
        "        df_shots[\"gameid\"] = pbp_data[\"game\"][\"gameId\"]  # Use gameId from the response\n",
        "        df_shots[\"actionId\"] = df_shots[\"actionNumber\"]\n",
        "        df_shots[\"period\"] = df_shots[\"period\"]\n",
        "        df_shots[\"clock_seconds\"] = df_shots[\"clock\"].apply(parse_clock)\n",
        "        df_shots = df_shots.dropna(subset=[\"clock_seconds\"])\n",
        "        # df_shots[\"gameclock\"] = (720 * df_shots[\"period\"]) - df_shots[\"clock_seconds\"]\n",
        "        df_shots[\"gameclock\"]=df_shots[\"clock_seconds\"]\n",
        "        df_shots[\"personId\"] = df_shots[\"personId\"]\n",
        "        df_shots[\"shotResult\"] = df_shots[\"shotResult\"].apply(lambda x: \"Made Shot\" if x == \"Made\" else \"Missed Shot\")\n",
        "        df_shots[\"shotDistance\"] = df_shots[\"shotDistance\"]\n",
        "\n",
        "\n",
        "        # Define columns to return (required + available optional columns)\n",
        "        return_cols = [\"gameid\", \"actionId\", \"period\", \"gameclock\", \"personId\", \"shotResult\", \"shotDistance\"]\n",
        "        available_optional_cols = [col for col in optional_cols if col in df_pbp.columns]\n",
        "        return_cols.extend(available_optional_cols)\n",
        "\n",
        "        return df_shots[return_cols]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching PBP data for gameid {gameid}: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNN1nU5-vxH4"
      },
      "outputs": [],
      "source": [
        "def load_sportvu_event(archive_path):\n",
        "    \"\"\"Load SportVU data from a .7z archive with one row per event.\"\"\"\n",
        "    # Validate file existence and type\n",
        "    if not os.path.exists(archive_path):\n",
        "        print(f\"Error: {archive_path} does not exist.\")\n",
        "        return None, None\n",
        "    if not os.path.isfile(archive_path):\n",
        "        print(f\"Error: {archive_path} is not a file.\")\n",
        "        return None, None\n",
        "    if not archive_path.endswith('.7z'):\n",
        "        print(f\"Error: {archive_path} does not end with .7z.\")\n",
        "        return None, None\n",
        "\n",
        "    # Check file size\n",
        "    file_size = os.path.getsize(archive_path)\n",
        "    if file_size < 1024:  # Arbitrary threshold for a valid .7z file\n",
        "        print(f\"Error: {archive_path} is too small ({file_size} bytes), likely corrupted.\")\n",
        "        return None, None\n",
        "\n",
        "    # Create a temporary directory for extraction\n",
        "    temp_extract_dir = os.path.join(TEMP_DIR, os.path.basename(archive_path).replace('.7z', ''))\n",
        "    os.makedirs(temp_extract_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Use 7z command to extract the .7z file\n",
        "        result = subprocess.run(\n",
        "            ['7z', 'x', archive_path, f'-o{temp_extract_dir}', '-y'],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Error extracting {archive_path}: {result.stderr}\")\n",
        "            return None, None\n",
        "\n",
        "        # Find the JSON file in the extracted contents\n",
        "        json_file = None\n",
        "        for root, _, files in os.walk(temp_extract_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.json'):\n",
        "                    json_file = os.path.join(root, file)\n",
        "                    break\n",
        "            if json_file:\n",
        "                break\n",
        "\n",
        "        if not json_file:\n",
        "            print(f\"No JSON file found in {archive_path}\")\n",
        "            return None, None\n",
        "\n",
        "        # Read the JSON file\n",
        "        with open(json_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        gameid = extract_gameid(data)\n",
        "        if not gameid:\n",
        "            print(f\"No gameid found in {archive_path}\")\n",
        "            return None, None\n",
        "\n",
        "        events = data[\"events\"]\n",
        "        event_data = []\n",
        "        for event in events:\n",
        "            event_id = event[\"eventId\"]\n",
        "            moments = event[\"moments\"]\n",
        "            # Skip events with no moments or invalid moments\n",
        "            if not moments or not isinstance(moments, list) or not all(isinstance(m, list) for m in moments):\n",
        "                continue\n",
        "            event_dict = {\n",
        "                \"eventId\": pd.to_numeric(event_id, errors='coerce').astype('int64'),\n",
        "                \"gameid\": gameid,\n",
        "                \"moments\": moments  # Store the entire moments list\n",
        "            }\n",
        "            event_data.append(event_dict)\n",
        "\n",
        "        if not event_data:\n",
        "            print(f\"No valid events found in {archive_path}\")\n",
        "            return None, None\n",
        "\n",
        "        sportvu_df = pd.DataFrame(event_data)\n",
        "        print(f\"Successfully parsed SportVU data from {archive_path}\")\n",
        "        return sportvu_df, gameid\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {archive_path}: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "    finally:\n",
        "        # Clean up the temporary directory\n",
        "        if os.path.exists(temp_extract_dir):\n",
        "            shutil.rmtree(temp_extract_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4SZkj8IpJDH"
      },
      "outputs": [],
      "source": [
        "def merge_sportvu_pbp(window=WINDOW):\n",
        "    \"\"\"Merge SportVU data with PBP data and extract sequences for shot events.\"\"\"\n",
        "    if not os.path.exists(OUTPUT_DIR):\n",
        "        os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "    # Create a fresh temporary directory\n",
        "    if os.path.exists(TEMP_DIR):\n",
        "        shutil.rmtree(TEMP_DIR)\n",
        "    os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "\n",
        "    archive_files = [f for f in os.listdir(INPUT_FILE_DIR) if f.endswith(\".7z\")]\n",
        "    if not archive_files:\n",
        "        print(\"No .7z files found in the input directory.\")\n",
        "        return None\n",
        "\n",
        "    sampled_files = random.sample(archive_files, min(NUM_MATCHES, len(archive_files)))\n",
        "    print(f\"Processing {len(sampled_files)} matches out of {len(archive_files)}\")\n",
        "\n",
        "    all_merged_data = []\n",
        "    shots_processed = 0\n",
        "\n",
        "    # Dictionary to store statistics for each game\n",
        "    games_stats = {}\n",
        "\n",
        "    # Report file path\n",
        "    report_path = os.path.join(OUTPUT_DIR, \"data_quality_report.txt\")\n",
        "\n",
        "    for archive_file in sampled_files:\n",
        "        if shots_processed >= MAX_SHOTS:\n",
        "            break\n",
        "\n",
        "        archive_path = os.path.join(INPUT_FILE_DIR, archive_file)\n",
        "        sportvu_df, gameid = load_sportvu_event(archive_path)\n",
        "        if sportvu_df is None or gameid is None:\n",
        "            continue\n",
        "\n",
        "        # Initialize game stats dictionary\n",
        "        games_stats[gameid] = {\n",
        "            \"archive_file\": archive_file,\n",
        "            \"events_count\": len(sportvu_df),\n",
        "            \"total_shots\": 0,\n",
        "            \"used_shots\": 0,\n",
        "            \"nan_gameclock\": 0,\n",
        "            \"nan_gameclock_rows\": 0,\n",
        "            \"all_nan_gameclock\": 0,\n",
        "            \"no_moments\": 0,\n",
        "            \"empty_df\": 0,\n",
        "            \"no_moments_in_window\": 0,\n",
        "            \"duplicate_frames\": 0,\n",
        "            \"duplicate_frames_dropped\": 0,\n",
        "            \"insufficient_length\": 0,\n",
        "            \"orig_frame_counts\": [],\n",
        "            \"final_frame_counts\": [],\n",
        "            \"insufficient_length_counts\": []\n",
        "        }\n",
        "\n",
        "        pbp_shots = load_pbp_shots(gameid)\n",
        "        if pbp_shots is None:\n",
        "            games_stats[gameid][\"pbp_shots_count\"] = 0\n",
        "            continue\n",
        "\n",
        "        games_stats[gameid][\"pbp_shots_count\"] = len(pbp_shots)\n",
        "\n",
        "        # Merge on actionId and eventId\n",
        "        merged = pd.merge(\n",
        "            pbp_shots,\n",
        "            sportvu_df,\n",
        "            left_on=[\"gameid\", \"actionId\"],\n",
        "            right_on=[\"gameid\", \"eventId\"],\n",
        "            how=\"inner\"\n",
        "        )\n",
        "\n",
        "        if merged.empty:\n",
        "            print(f\"No matching shots found for gameid: {gameid}\")\n",
        "            games_stats[gameid][\"matched_shots\"] = 0\n",
        "            continue\n",
        "\n",
        "        games_stats[gameid][\"matched_shots\"] = len(merged)\n",
        "\n",
        "        # Extract sequences for each shot\n",
        "        merged_data = []\n",
        "        for idx, row in merged.iterrows():\n",
        "            target_gameclock = row[\"gameclock\"]\n",
        "            moments = row[\"moments\"]\n",
        "            ball_seq, players_seq, game_stats = extract_sequence(moments, target_gameclock, row[\"period\"], window, games_stats[gameid])\n",
        "            games_stats[gameid] = game_stats  # Update with latest stats\n",
        "\n",
        "            if ball_seq is None or players_seq is None:\n",
        "                continue\n",
        "\n",
        "            shot_data = row.drop([\"eventId\", \"moments\"]).to_dict()\n",
        "            shot_data[\"ball_seq\"] = ball_seq\n",
        "            shot_data[\"players_seq\"] = players_seq\n",
        "            merged_data.append(shot_data)\n",
        "\n",
        "        if not merged_data:\n",
        "            print(f\"No sequences extracted for gameid: {gameid}\")\n",
        "            continue\n",
        "\n",
        "        game_df = pd.DataFrame(merged_data)\n",
        "        all_merged_data.append(game_df)\n",
        "        shots_processed += len(game_df)\n",
        "        print(f\"Processed {len(game_df)} shots for gameid: {gameid}. Total shots: {shots_processed}\")\n",
        "\n",
        "        # Update game stats with final count of shots actually used\n",
        "        games_stats[gameid][\"final_shots_used\"] = len(game_df)\n",
        "\n",
        "    # Write stats to report file\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(\"NBA SportVU Data Quality Report\\n\")\n",
        "        f.write(\"==============================\\n\\n\")\n",
        "        f.write(f\"Total games processed: {len(games_stats)}\\n\")\n",
        "        f.write(f\"Total shots processed: {shots_processed}\\n\\n\")\n",
        "        f.write(\"Per-Game Statistics:\\n\")\n",
        "        f.write(\"-------------------\\n\\n\")\n",
        "\n",
        "        for gameid, stats in games_stats.items():\n",
        "            f.write(f\"GameID: {gameid} (File: {stats['archive_file']})\\n\")\n",
        "            f.write(f\"  SportVU events: {stats['events_count']}\\n\")\n",
        "            f.write(f\"  PBP shots: {stats.get('pbp_shots_count', 0)}\\n\")\n",
        "            f.write(f\"  Matched shots: {stats.get('matched_shots', 0)}\\n\")\n",
        "            f.write(f\"  Total shots examined: {stats.get('total_shots', 0)}\\n\")\n",
        "            f.write(f\"  Used shots: {stats.get('used_shots', 0)} (final: {stats.get('final_shots_used', 0)})\\n\")\n",
        "            f.write(f\"  Dropped shots breakdown:\\n\")\n",
        "            f.write(f\"    - No moments data: {stats.get('no_moments', 0)}\\n\")\n",
        "            f.write(f\"    - Empty DataFrame: {stats.get('empty_df', 0)}\\n\")\n",
        "            f.write(f\"    - NaN gameclock: {stats.get('nan_gameclock', 0)} (rows: {stats.get('nan_gameclock_rows', 0)})\\n\")\n",
        "            f.write(f\"    - All NaN gameclock: {stats.get('all_nan_gameclock', 0)}\\n\")\n",
        "            f.write(f\"    - No moments in window: {stats.get('no_moments_in_window', 0)}\\n\")\n",
        "            f.write(f\"    - Duplicate frames: {stats.get('duplicate_frames', 0)} (frames: {stats.get('duplicate_frames_dropped', 0)})\\n\")\n",
        "            f.write(f\"    - Insufficient sequence length: {stats.get('insufficient_length', 0)}\\n\")\n",
        "\n",
        "            # Calculate averages if data is available\n",
        "            if stats.get('orig_frame_counts'):\n",
        "                avg_orig_frames = sum(stats['orig_frame_counts']) / len(stats['orig_frame_counts'])\n",
        "                f.write(f\"  Average original frames per shot: {avg_orig_frames:.2f}\\n\")\n",
        "\n",
        "            if stats.get('final_frame_counts'):\n",
        "                avg_final_frames = sum(stats['final_frame_counts']) / len(stats['final_frame_counts'])\n",
        "                f.write(f\"  Average final frames per shot: {avg_final_frames:.2f}\\n\")\n",
        "\n",
        "            if stats.get('insufficient_length_counts'):\n",
        "                avg_insuff_frames = sum(stats['insufficient_length_counts']) / len(stats['insufficient_length_counts'])\n",
        "                f.write(f\"  Average frames in insufficient sequences: {avg_insuff_frames:.2f}\\n\")\n",
        "\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        # Add summary statistics\n",
        "        f.write(\"\\nSummary Statistics:\\n\")\n",
        "        f.write(\"------------------\\n\")\n",
        "        total_events = sum(stats['events_count'] for stats in games_stats.values())\n",
        "        total_pbp_shots = sum(stats.get('pbp_shots_count', 0) for stats in games_stats.values())\n",
        "        total_matched = sum(stats.get('matched_shots', 0) for stats in games_stats.values())\n",
        "        total_examined = sum(stats.get('total_shots', 0) for stats in games_stats.values())\n",
        "        total_used = sum(stats.get('final_shots_used', 0) for stats in games_stats.values())\n",
        "\n",
        "        f.write(f\"Total SportVU events: {total_events}\\n\")\n",
        "        f.write(f\"Total PBP shots: {total_pbp_shots}\\n\")\n",
        "        f.write(f\"Total matched shots: {total_matched}\\n\")\n",
        "        f.write(f\"Total shots examined: {total_examined}\\n\")\n",
        "        f.write(f\"Total shots used in final dataset: {total_used}\\n\")\n",
        "        f.write(f\"Overall data utilization rate: {(total_used / total_examined * 100):.2f}%\\n\")\n",
        "\n",
        "    print(f\"Data quality report written to {report_path}\")\n",
        "\n",
        "    if not all_merged_data:\n",
        "        print(\"No data merged after processing all matches.\")\n",
        "        return None\n",
        "\n",
        "    final_df = pd.concat(all_merged_data, ignore_index=True)\n",
        "    final_df[\"target\"] = (final_df[\"shotResult\"] == \"Made Shot\").astype(int)\n",
        "\n",
        "    # Log the distribution of Made vs. Missed shots before splitting\n",
        "    made_count = (final_df[\"target\"] == 1).sum()\n",
        "    missed_count = (final_df[\"target\"] == 0).sum()\n",
        "    print(f\"Before splitting - Made shots: {made_count}, Missed shots: {missed_count}, Proportion Made: {made_count / (made_count + missed_count):.2f}\")\n",
        "\n",
        "    # Stratified split to ensure balanced Made/Missed shots\n",
        "    train_df, temp_df = train_test_split(final_df, test_size=0.3, stratify=final_df[\"target\"], random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"target\"], random_state=42)\n",
        "\n",
        "    # Log the distribution in each split\n",
        "    for split_name, split_df in [(\"Training\", train_df), (\"Validation\", val_df), (\"Test\", test_df)]:\n",
        "        made_count = (split_df[\"target\"] == 1).sum()\n",
        "        missed_count = (split_df[\"target\"] == 0).sum()\n",
        "        print(f\"{split_name} set - Made shots: {made_count}, Missed shots: {missed_count}, Proportion Made: {made_count / (made_count + missed_count):.2f}\")\n",
        "\n",
        "    train_df.to_csv(os.path.join(OUTPUT_DIR, \"train.csv\"), index=False)\n",
        "    val_df.to_csv(os.path.join(OUTPUT_DIR, \"val.csv\"), index=False)\n",
        "    test_df.to_csv(os.path.join(OUTPUT_DIR, \"test.csv\"), index=False)\n",
        "\n",
        "    # Add dataset statistics to report\n",
        "    with open(report_path, 'a') as f:\n",
        "        f.write(\"\\nFinal Dataset Statistics:\\n\")\n",
        "        f.write(\"------------------------\\n\")\n",
        "        f.write(f\"Total dataset size: {len(final_df)}\\n\")\n",
        "        f.write(f\"Made shots: {made_count} ({made_count / len(final_df) * 100:.2f}%)\\n\")\n",
        "        f.write(f\"Missed shots: {missed_count} ({missed_count / len(final_df) * 100:.2f}%)\\n\\n\")\n",
        "\n",
        "        f.write(\"Dataset Splits:\\n\")\n",
        "        f.write(f\"  Training set: {len(train_df)} ({len(train_df) / len(final_df) * 100:.2f}%)\\n\")\n",
        "        f.write(f\"  Validation set: {len(val_df)} ({len(val_df) / len(final_df) * 100:.2f}%)\\n\")\n",
        "        f.write(f\"  Test set: {len(test_df)} ({len(test_df) / len(final_df) * 100:.2f}%)\\n\")\n",
        "\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvX1cKzrpJDJ"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Run the Pipeline\n",
        "merged_df = merge_sportvu_pbp(window=WINDOW)\n",
        "print(\"Pipeline completed.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}